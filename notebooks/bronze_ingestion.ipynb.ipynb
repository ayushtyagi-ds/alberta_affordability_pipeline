{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da47b9c7-227c-4543-8d6e-b4b5525af463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Bronze Layer Ingestion (Alberta Economic Monitor)\n",
    "\n",
    "**Objective:** Ingest all 5 clean, raw data files from the secure Volume (`/Volumes/mycatalog/default/bronze_data/`) and create our official Bronze tables.\n",
    "\n",
    "**Process:** This code reads each CSV, infers the schema, and saves it as a Delta table. The `.mode(\"overwrite\")` command ensures any old, broken tables are replaced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "243a68dc-c58d-42ee-ab5f-c4d6b5ff3038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.1 Ingest Wages\n",
    "\n",
    "- **Source:** `wages_raw.csv`\n",
    "- **Output:** `mycatalog.default.wages_bronze`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ea64545-c44d-4c08-97b7-0a4c868d0a6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. WAGES INGESTION ---\n",
    "# --- 1. WAGES INGESTION ---\n",
    "\n",
    "file_path = \"/Volumes/mycatalog/default/bronze_data/Wages_raw.csv\"\n",
    "table_name = \"mycatalog.default.Wages_bronze\"\n",
    "\n",
    "print(f\"Reading wages data from: {file_path}\")\n",
    "\n",
    "\n",
    "# Read the CSV\n",
    "raw_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(file_path)\n",
    ")\n",
    "\n",
    "# Sanitize column names: replace spaces and special characters with underscores\n",
    "for col in raw_df.columns:\n",
    "    new_col = (\n",
    "        col.replace(\" \", \"_\")\n",
    "        .replace(\",\", \"_\")\n",
    "        .replace(\"(\", \"\")\n",
    "        .replace(\")\", \"\")\n",
    "        .replace(\";\", \"_\")\n",
    "        .replace(\"{\", \"\")\n",
    "        .replace(\"}\", \"\")\n",
    "        .replace(\"\\n\", \"\")\n",
    "        .replace(\"\\t\", \"\")\n",
    "        .replace(\"=\", \"_\")\n",
    "    )\n",
    "    raw_df = raw_df.withColumnRenamed(col, new_col)\n",
    "\n",
    "# Drop the table if it exists\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "\n",
    "# Write to Delta\n",
    "raw_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "display(spark.table(table_name).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a695585-e2a4-4744-9048-9dd63ce3aa37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.2 Ingest CPI (Inflation)\n",
    "\n",
    "- **Source:** `cpi_raw.csv`\n",
    "- **Output:** `mycatalog.default.cpi_bronze`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42340ce9-0639-4469-bd70-e3ceff532759",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 2. CPI INGESTION ---\n",
    "\n",
    "file_path = \"/Volumes/mycatalog/default/bronze_data/CPI.csv\"\n",
    "table_name = \"mycatalog.default.cpi_bronze\"\n",
    "\n",
    "print(f\"Reading CPI data from: {file_path}\")\n",
    "\n",
    "# Read the CSV\n",
    "raw_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(file_path)\n",
    ")\n",
    "\n",
    "# Sanitize column names: replace invalid characters with underscores\n",
    "def sanitize_col(col):\n",
    "    invalid_chars = [' ', ',', ';', '{', '}', '(', ')', '\\n', '\\t', '=']\n",
    "    for ch in invalid_chars:\n",
    "        col = col.replace(ch, '_')\n",
    "    return col\n",
    "\n",
    "sanitized_cols = [sanitize_col(col) for col in raw_df.columns]\n",
    "\n",
    "# Ensure column names are unique\n",
    "from collections import Counter\n",
    "\n",
    "def make_unique(cols):\n",
    "    counts = Counter()\n",
    "    new_cols = []\n",
    "    for col in cols:\n",
    "        counts[col] += 1\n",
    "        if counts[col] == 1:\n",
    "            new_cols.append(col)\n",
    "        else:\n",
    "            new_cols.append(f\"{col}_{counts[col]}\")\n",
    "    return new_cols\n",
    "\n",
    "unique_cols = make_unique(sanitized_cols)\n",
    "raw_df = raw_df.toDF(*unique_cols)\n",
    "\n",
    "# Drop the table if it exists\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "\n",
    "# Write to Delta\n",
    "raw_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "display(spark.table(table_name).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be74ee49-97f6-4a67-aa52-89cb5eedf134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## 1.3 Ingest Rent (Housing)\n",
    "\n",
    "- **Source:** `download.csv`\n",
    "- **Output:** `mycatalog.default.rent_bronze`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94711b2d-ac2b-42f9-87cc-ad074a031dac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Drop the existing table if it exists\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    DROP TABLE IF EXISTS mycatalog.default.rent_bronze\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "file_path = \"/Volumes/mycatalog/default/bronze_data/download.csv\"\n",
    "\n",
    "rent_bronze = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .load(file_path)\n",
    ")\n",
    "\n",
    "rent_bronze.write.format(\"delta\") \\\n",
    "    .option(\"delta.columnMapping.mode\", \"name\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"mycatalog.default.rent_bronze\")\n",
    "\n",
    "display(spark.table(\"mycatalog.default.rent_bronze\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "677e6403-5f95-42a5-97fa-815cbba3ae05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.4 Ingest GDP\n",
    "\n",
    "- **Source:** `gdp_raw.csv`\n",
    "- **Output:** `mycatalog.default.gdp_bronze`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25e60fff-a180-42de-b2fb-85670ec8378d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 4. GDP INGESTION ---\n",
    "\n",
    "file_path = \"/Volumes/mycatalog/default/bronze_data/gdp.csv\"\n",
    "table_name = \"mycatalog.default.gdp_bronze\"\n",
    "\n",
    "print(f\"Reading GDP data from: {file_path}\")\n",
    "\n",
    "# Read the CSV\n",
    "raw_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(file_path)\n",
    ")\n",
    "\n",
    "# Sanitize and make column names unique\n",
    "from collections import Counter\n",
    "\n",
    "def sanitize_col(col):\n",
    "    invalid_chars = [' ', ',', ';', '{', '}', '(', ')', '\\n', '\\t', '=']\n",
    "    for ch in invalid_chars:\n",
    "        col = col.replace(ch, '_')\n",
    "    return col\n",
    "\n",
    "sanitized_cols = [sanitize_col(col) for col in raw_df.columns]\n",
    "\n",
    "def make_unique(cols):\n",
    "    counts = Counter()\n",
    "    new_cols = []\n",
    "    for col in cols:\n",
    "        counts[col] += 1\n",
    "        if counts[col] == 1:\n",
    "            new_cols.append(col)\n",
    "        else:\n",
    "            new_cols.append(f\"{col}_{counts[col]}\")\n",
    "    return new_cols\n",
    "\n",
    "unique_cols = make_unique(sanitized_cols)\n",
    "raw_df = raw_df.toDF(*unique_cols)\n",
    "\n",
    "# Drop the table if it exists\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "\n",
    "# Write to Delta\n",
    "raw_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"✅ Successfully created table: {table_name}\")\n",
    "display(spark.table(table_name).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb1a4bbb-5bc1-4ab1-bfb7-5cf106ea57da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.5 Ingest Unemployment\n",
    "\n",
    "- **Source:** `unemployment_raw.csv`\n",
    "- **Output:** `mycatalog.default.unemployment_bronze`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a3b9759-d1ab-430e-9b31-0007b9f2d8ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 5. UNEMPLOYMENT INGESTION ---\n",
    "\n",
    "file_path = \"/Volumes/mycatalog/default/bronze_data/Unemployment.csv\"\n",
    "table_name = \"mycatalog.default.unemployment_bronze\"\n",
    "\n",
    "print(f\"Reading unemployment data from: {file_path}\")\n",
    "\n",
    "raw_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(file_path)\n",
    ")\n",
    "\n",
    "# Sanitize column names to remove invalid characters\n",
    "def sanitize_col(col):\n",
    "    invalid_chars = [' ', ',', ';', '{', '}', '(', ')', '\\n', '\\t', '=']\n",
    "    for ch in invalid_chars:\n",
    "        col = col.replace(ch, '_')\n",
    "    return col\n",
    "\n",
    "sanitized_cols = [sanitize_col(col) for col in raw_df.columns]\n",
    "raw_df = raw_df.toDF(*sanitized_cols)\n",
    "\n",
    "# Drop the table if it exists\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "\n",
    "raw_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"✅ Successfully created table: {table_name}\")\n",
    "display(spark.table(table_name).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea7a1939-07fc-4972-8d6e-26b6ccb76d15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.6 Ingest Wages\n",
    "\n",
    "- **Source:** `Wages_raw.csv`\n",
    "- **Output:** `mycatalog.default.wages_bronze`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1480cdba-f82a-49a0-b1b6-10e29e92f889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# List files in your bronze_data Volume to confirm the path / file\n",
    "display(dbutils.fs.ls(\"/Volumes/mycatalog/default/bronze_data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76709028-668b-46d3-86df-1369fb011ed1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "raw_path = \"/Volumes/mycatalog/default/bronze_data/Wages_raw.csv\"\n",
    "\n",
    "wages_raw_df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")          \n",
    "    .option(\"header\", \"true\")\n",
    "    .load(raw_path)\n",
    ")\n",
    "\n",
    "print(\"Wages_raw sample:\")\n",
    "display(wages_raw_df.limit(10))\n",
    "\n",
    "print(\"Wages_raw schema:\")\n",
    "wages_raw_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54aae38c-f4d2-409e-a3e1-9e5d4f38fec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import re\n",
    "\n",
    "def sanitize_column(col):\n",
    "    return re.sub(r'[ ,;{}()\\n\\t=]', '_', col)\n",
    "\n",
    "wages_raw_df = wages_raw_df.toDF(\n",
    "    *[sanitize_column(col) for col in wages_raw_df.columns]\n",
    ")\n",
    "\n",
    "wages_bronze = \"mycatalog.default.wages_bronze\"\n",
    "\n",
    "wages_raw_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(wages_bronze)\n",
    "\n",
    "print(f\"✅ Recreated Bronze table: {wages_bronze}\")\n",
    "\n",
    "display(spark.table(wages_bronze).limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26a87cb1-619b-43bf-8b63-c5b711ac021f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.7 Final Verification\n",
    "\n",
    "Run this cell to confirm all 5 Bronze tables exist in our catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f6e5845-bd80-46d6-b3cb-11ffc71a86ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW TABLES IN mycatalog.default LIKE '*_bronze';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a677f6b-c8d2-4743-a0e6-d621e8e51b86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7800561990676649,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_ingestion.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
